{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46613504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba2fdf61",
   "metadata": {},
   "source": [
    "## 1. Linear regression vs ANN\n",
    "   #  Manual feature engineering\n",
    "   #  How ANN engineers features from inputs?\n",
    "   \n",
    "\n",
    "## Neural Network Basics\n",
    "# 1. Introduction to Keras\n",
    " # 2.Keras Sequential NN model\n",
    "\n",
    "# Compare the effectiveness ANN ultiple Regresiom.\n",
    "\n",
    "# Demonnstrate that a better understanding of the problem to be solved allows engineering features that can enriches the inputs information and enhance the performance of ANN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d41085",
   "metadata": {},
   "source": [
    "## Keras created by Google as a high-level neural network API. \n",
    " # It simplies deep learning and artificial intelligence application. \n",
    "  # It runs on top of a number of lower-level libraries, used as backends, including TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f07e413",
   "metadata": {},
   "source": [
    "## 1. Linear regression vs ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09df9417",
   "metadata": {},
   "source": [
    "# Import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22bf1f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First we need to update numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eda93bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.23.4\n",
      "  Using cached numpy-1.23.4-cp38-cp38-macosx_10_9_x86_64.whl (18.1 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "daal4py 2021.6.0 requires daal==2021.4.0, which is not installed.\n",
      "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.23.4 which is incompatible.\n",
      "scipy 1.7.3 requires numpy<1.23.0,>=1.16.5, but you have numpy 1.23.4 which is incompatible.\n",
      "bokeh 2.4.3 requires typing-extensions>=3.10.0, but you have typing-extensions 3.7.4.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.23.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy==1.23.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2342c691",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "this version of pandas is incompatible with numpy < 1.20.3\nyour numpy version is 1.19.5.\nPlease upgrade numpy to >= 1.20.3 to use this pandas version",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msm\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/yes/lib/python3.8/site-packages/pandas/__init__.py:22\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _hard_dependencies, _dependency, _missing_dependencies\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# numpy compat\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_numpy_dev \u001b[38;5;28;01mas\u001b[39;00m _is_numpy_dev  \u001b[38;5;66;03m# pyright: ignore # noqa:F401\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_libs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hashtable \u001b[38;5;28;01mas\u001b[39;00m _hashtable, lib \u001b[38;5;28;01mas\u001b[39;00m _lib, tslib \u001b[38;5;28;01mas\u001b[39;00m _tslib\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/yes/lib/python3.8/site-packages/pandas/compat/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_typing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m F\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     19\u001b[0m     is_numpy_dev,\n\u001b[1;32m     20\u001b[0m     np_version_under1p21,\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     23\u001b[0m     pa_version_under1p01,\n\u001b[1;32m     24\u001b[0m     pa_version_under2p0,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     pa_version_under9p0,\n\u001b[1;32m     32\u001b[0m )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/yes/lib/python3.8/site-packages/pandas/compat/numpy/__init__.py:22\u001b[0m\n\u001b[1;32m     18\u001b[0m     np_percentile_argname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterpolation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _nlv \u001b[38;5;241m<\u001b[39m Version(_min_numpy_ver):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis version of pandas is incompatible with numpy < \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_min_numpy_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour numpy version is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_np_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease upgrade numpy to >= \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_min_numpy_ver\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to use this pandas version\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m     )\n\u001b[1;32m     29\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_np_version\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_numpy_dev\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     33\u001b[0m ]\n",
      "\u001b[0;31mImportError\u001b[0m: this version of pandas is incompatible with numpy < 1.20.3\nyour numpy version is 1.19.5.\nPlease upgrade numpy to >= 1.20.3 to use this pandas version"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ccb756",
   "metadata": {},
   "source": [
    "# Let us list files in our working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5947be67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cad7e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 =  pd.read_csv('top10_ret_volume_2023-05-24.csv')\n",
    "d1  =  pd.read_csv('top10_ret_volume_2023-06-17.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c143eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "d0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c8ce79",
   "metadata": {},
   "outputs": [],
   "source": [
    "d0_ = d0 [ d0['day'] < '2023-05-23']\n",
    "d1_ = d1 [ d1['day'] > '2023-05-22']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2549f6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data0 = pd.concat([d0_,d1_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19588f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "data0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a942d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data0[['symbol','etf' , 'day' , 'time' ]].describe()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63c5965",
   "metadata": {},
   "outputs": [],
   "source": [
    "data0['symbol'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40c1a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "data0['etf'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6f9b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "data0['time'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc73d5b4",
   "metadata": {},
   "source": [
    "## we have 5-minutes return and volume data for 53, for each symbol, daysstarting at 9:30 and ending at 15:55\n",
    "# Let us filer out time = 16:00:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47143c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data0[data0['time']< '16:00:00']\n",
    "print ( data0.shape, data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107ef7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['symbol'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493641a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_sym_day_volume = data.groupby(['symbol','day'])['volume'].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58900d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_sym_volume = by_sym_day_volume.groupby('symbol').median().reset_index()\n",
    "by_sym_volume = by_sym_volume.sort_values('volume', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780bafaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "syms_ = by_sym_volume.head(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2980c6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "syms=list( syms_['symbol'] )\n",
    "print(syms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3715fa0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fb2d1b4",
   "metadata": {},
   "source": [
    "## In our analysis we will focus the above 30 stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd1fd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "syms0= data[data['symbol'].isin(syms)]\n",
    "print( data0.shape, syms0.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85264332",
   "metadata": {},
   "outputs": [],
   "source": [
    "syms0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292236b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sym0.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e5b7a9",
   "metadata": {},
   "source": [
    "## let us focus on a subset of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072392bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "syms0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba29955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "292ba75d",
   "metadata": {},
   "source": [
    "## Note that,\n",
    " # ret          is the stock's return over the 5-minutes ending at time t\n",
    " # volume  is the stock's volume over the 5-minutes ending at time t\n",
    "## next_ret   is the stock's return over the 5-minutes starting at time t\n",
    " # At time t,  ret and volume are know, but next_ret is not know."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2309cc3b",
   "metadata": {},
   "source": [
    "## Suppose that we want to predict next_ret at time t using information at t ( i.e. [ret, volume])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303e61d3",
   "metadata": {},
   "source": [
    "## let us look at the following correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4a46ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "syms0[['next_ret' , 'ret' , 'spy_ret', 'etf_ret' , 'volume']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67707eb3",
   "metadata": {},
   "source": [
    "## Obviously, predicting 'next_ret' is very challenging task\n",
    " # Remember the Efficient Market Hypothesis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d2602a",
   "metadata": {},
   "source": [
    "\n",
    "# The Efficient Market Hypothesis (EMH) implies that past information, including past returns, do not help predicting future return.\n",
    "\n",
    " # or the best price prediction of future price is the current price. \n",
    "  # This does not mean that then next price will be the same as current price. It means that unexpected events will cause such price changes.\n",
    "  \n",
    "  # Without the knowledge of these unexpected events, the current price contains all the information about the future price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6771532",
   "metadata": {},
   "source": [
    "## Nevertheless, day traders do try. \n",
    "\n",
    " # Remember,\n",
    "\n",
    " # ( i )They have access to more information that we do, like book imbalances across markets;\n",
    " \n",
    "  # ( i i ) While markets are efficient most of the time, short-lived pockets of inefficiencies can exist;\n",
    "  \n",
    "   # ( i i i ) better speed of execution, cheaper execution cost can make trading based such prediction a profitable effort.  successful trading requires \n",
    "   # (a) locating rare profitable opportunity, \n",
    "   # (b) monitoring inventory of stocks and,\n",
    "   # (c) adjusting a stock trading trigers based on its prediction, its inventory, and its impact on the risk of the inventory portfolio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3eb094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31e16e8a",
   "metadata": {},
   "source": [
    "# Here we will focus on,\n",
    " # (i) using Linear regression\n",
    " # (ii) examine wht ANN can add to the prediction\n",
    " # (iii) Discuss whether clever feature engineering can help prediction without and with ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1061787",
   "metadata": {},
   "source": [
    "## 1. Simple regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6192254",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY0= sym0[['ret', 'volume', 'next_ret']].dropna()\n",
    "X0= XY0[['ret', 'volume']]\n",
    "y = XY0['next_ret']\n",
    "X0 = sm.add_constant(X0)\n",
    "lm0 = sm.OLS(y, X0).fit()\n",
    "lm0.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d73be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.sqrt( lm0.rsquared )\n",
    "round( r, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a230e3",
   "metadata": {},
   "source": [
    "## Not a great predictive model. Still we have a significant F stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb7873e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9fc071b",
   "metadata": {},
   "source": [
    "## the above regression assumes that ret and volume has additive effects.\n",
    " # What if there are non-linear effects\n",
    " # We call such effects interactions. For example, \n",
    " # (i ) What if the slope of 'ret'with respect to 'next_ret' is different. For example, a strong move may cause reversion while mediocre move does not cause reversion\n",
    " # (ii ) What if strong volume and strong 'ret' has distinct impact on 'next_ret'.\n",
    " # ( iii ) what if a non-linear fumction of volume provide a predictive power that volume itself can't provide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e77135e",
   "metadata": {},
   "source": [
    "## First let us examine the variables that capture the information on which we will base our prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b75ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.6f' % x)\n",
    "syms0[['ret', 'volume', 'next_ret']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28034459",
   "metadata": {},
   "source": [
    "## Note that return and volume have different units. Some times standarizing the data can help improve prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73dd1e5",
   "metadata": {},
   "source": [
    "# let us look at the distribution of 'ret'. Here we are importing seaborn and matplotlib and graphing some distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71253e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fba895",
   "metadata": {},
   "outputs": [],
   "source": [
    "g= sns.displot(sym, x=\"ret\" , kde=True)\n",
    "g.fig.set_figwidth(6)\n",
    "g.fig.set_figheight(3)\n",
    "plt.xlim(0, 150000)\n",
    "plt.xlim(-0.02, 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d08c87",
   "metadata": {},
   "source": [
    "## Very much normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a3dd8b",
   "metadata": {},
   "source": [
    "## On the other hand the distribution of volume is very skewed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93024c92",
   "metadata": {},
   "source": [
    "## First let us filter out zero volume data and then graph the distribution of volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb592dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "syms= syms0[syms0['volume']> 0]\n",
    "g = sns.displot(sym, x=\"volume\" , kde=True)\n",
    "g.fig.set_figwidth(6)\n",
    "g.fig.set_figheight(3)\n",
    "plt.xlim(syms['volume'].min(), syms['volume'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d757e993",
   "metadata": {},
   "source": [
    "## Here we limit the upper volume to its 95th quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c609be90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "g = sns.displot(syms, x=\"volume\" , kde=True)\n",
    "g.fig.set_figwidth(6)\n",
    "g.fig.set_figheight(3)\n",
    "plt.xlim(syms['volume'].min(), syms['volume'].quantile(0.95))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f8614a",
   "metadata": {},
   "source": [
    "## Very skewed distributution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1790af62",
   "metadata": {},
   "source": [
    "## Standarization helps make all variables have the same scale.\n",
    " # some times we may want to transformsom variables so that their distributions are similar.\n",
    " \n",
    " # Let us introduce two transformations of volume by creating two columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ac41fd",
   "metadata": {},
   "source": [
    "# 1. This one creates a transformed variable witha zero mean and std=1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed0c10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "syms['standaized_volume']= ( syms['volume'] - syms['volume'].mean() )/ syms['volume'].std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342bc277",
   "metadata": {},
   "source": [
    "## Let us see whether this distribution impacts skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be89c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "g= sns.displot(syms, x=\"standaized_volume\" , kde=True)\n",
    "g.fig.set_figwidth(6)\n",
    "g.fig.set_figheight(5)\n",
    "plt.xlim(0, 50)\n",
    "plt.ylim(0, syms['standaized_volume'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee39259",
   "metadata": {},
   "source": [
    "## THe above destribution changes location and scale unites. However, it does not take care of skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3203761f",
   "metadata": {},
   "source": [
    "## Another transformation is taking natural log of volume. However, the log function is not defined for a zero argument. Let us filter out cases with zero volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b78a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "syms['log_volume'] =  np.log(syms['volume'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81fa9ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g=sns.displot(syms, x=\"log_volume\" , kde=True)\n",
    "g.fig.set_figwidth(6)\n",
    "g.fig.set_figheight(3)\n",
    "plt.xlim(syms['log_volume'].min(), syms['log_volume'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d46e5a",
   "metadata": {},
   "source": [
    "## Closer to being symmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d5d45f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f12e4f89",
   "metadata": {},
   "source": [
    "## Some \"manual\" methods to capture non-linear effect:\n",
    "\n",
    " # I say manual because ANN is designed to capture non-linearity in the input variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fde2c6f",
   "metadata": {},
   "source": [
    "## Inntuitively, next_ret may response to strong up return differently that to mediocre return or strong negative return. Like the case of a pendulum, a weak force may have little impact, while a strong force results in an opposite direction swing. How can we test for that?\n",
    "\n",
    "# We can engineer two features from the variable 'ret': (i) Capture the current return, 'ret' , when it is only strong up; and (ii) capture it when it is only strong down.\n",
    "\n",
    "# we need threshold to determine up oand down. here is an example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994de086",
   "metadata": {},
   "source": [
    "## examine the following two variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770e55f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "q90 = syms['ret'].quantile(0.90)\n",
    "q10 = syms['ret'].quantile(0.10)\n",
    "syms['ret_q90'] = np.where(syms['ret'] > q90 , syms['ret']- q90 ,0 )\n",
    "syms['ret_q10'] = np.where(syms['ret'] < q10 , syms['ret']- q10, 0 )\n",
    "syms[['ret' , 'ret_q90' , 'ret_q10']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9405e841",
   "metadata": {},
   "outputs": [],
   "source": [
    "syms[['next_ret','ret' , 'ret_q10' , 'ret_q90']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a932294",
   "metadata": {},
   "source": [
    "## Note how next_ret correlate differently between the three variables. Reversion is less pronounced for large current return. This is because sometime reversion happens and other time momentum happens with strong move. Some cancelation of reversion takes place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7011e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a461afbb",
   "metadata": {},
   "source": [
    "## Introducing Activation function ReLU ( Rectified Linear Unit). Recently most ML algorithms use such activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3086f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    y = np.maximum(0,x)\n",
    "    return(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599e4f6f",
   "metadata": {},
   "source": [
    "## Here is a graph of this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e942f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= np.linspace(-0.01, 0.01, 100)\n",
    "y = relu(x)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(list(x),list(y) )\n",
    "plt.title ( \"y = relu(x)\" ) \n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4452fdc",
   "metadata": {},
   "source": [
    "## Here is a graph of the first engineered features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e141bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "syms.plot(kind= 'scatter' , x= 'ret', y='ret_q90', color='b', label = 'ret_q90 ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fce0775",
   "metadata": {},
   "source": [
    "## This is exactly relu(ret - q90 ). Let us verify that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84b9162",
   "metadata": {},
   "outputs": [],
   "source": [
    "syms['relu_ret - q90'] = (syms['ret']-q90).apply(relu)\n",
    "print( syms[['ret_q90', 'relu_ret - q90']].agg(['mean', 'std']) )\n",
    "syms[['ret_q90', 'relu_ret - q90']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a864cf05",
   "metadata": {},
   "source": [
    "## This shows the two variables are identical. It also shows how the activstion function can be use to capture non linearity of any input any linear combinations of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7a9ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "syms.plot(kind= 'scatter' , x= 'ret', y='ret_q10', color='r', label = 'ret_q10 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3776bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "syms['r-act']= syms['ret'] -  (syms['ret']-q10).apply(relu)\n",
    "syms.plot(kind= 'scatter' , x= 'ret', y='r-act', color='g', label = 'rrt - activation ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fa911a",
   "metadata": {},
   "source": [
    "## So relu activation function can also generate the second engineered feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b53ef9",
   "metadata": {},
   "source": [
    "## By applying ReLU activiation function on a variable or a linear combination of variables we are engineering features that capture non-linear relationship between inputs and the out put variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f52ae0b",
   "metadata": {},
   "source": [
    "## What is ANN slow at or not efficient with is capturing interaction of a product effect between inputs, like ret*volume\n",
    "\n",
    "# There are two ways to handle that;\n",
    " # (i) creating a new input equal to the product of the two inputs\n",
    " # ( ii) Apply a log transformation for the product. According to the log function,\n",
    " # log( a * b ) = log(a) + log(b)    if a > 0 and b > 0\n",
    " # If each term is positive, the log of the product is the sum of the logs. \n",
    "  # In case of the variable 'ret' , we face the problem that ret can be negative.\n",
    "  \n",
    "   #  However, if we can change ret to 1+ret. Using (1 +ret) instead of ret does not sacrifice any information. But it allows capturing the the interation, it neutralizes any impact of the skewed distribution of volume and helps ML algorithm learn about the effect of this interaction.\n",
    "   # Another function of ret is exp(ret). If instead of ret*volume we have exp(ret)*volume, then \n",
    "   # log ( exp(ret)*volume ) = log(exp(ret) + log(volume) = ret + log(volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4133075a",
   "metadata": {},
   "outputs": [],
   "source": [
    "syms['volume*ret']=syms['volume']*syms['ret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409de6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "syms.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681ee4d9",
   "metadata": {},
   "source": [
    "# Let us define the following df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478024bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vrs= ['symbol', 'etf', 'day', 'time',  \n",
    "       'next_ret', 'ret', 'spy_ret', 'etf_ret',\n",
    "      'volume' , 'standaized_volume', 'log_volume','volume*ret',\n",
    "      'ret_q90', 'ret_q10']\n",
    "syms1= syms[vrs]\n",
    "syms1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cde704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sym1_normalized = (syms1 - syms1.mean() )/syms1.std()\n",
    "sym1_normalized.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7e4357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2b09ce2",
   "metadata": {},
   "source": [
    "## Here is a regression model with all the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ada9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57255a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( vrs[4:] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc978da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_syms1 = syms1[vrs[4:]].dropna()\n",
    "X= XY_syms1[vrs[5:]]\n",
    "y = XY_syms1['next_ret']\n",
    "X = sm.add_constant(X)\n",
    "lm1 = sm.OLS(y, X).fit()\n",
    "lm1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb34a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(lm1.rsquared)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee188154",
   "metadata": {},
   "source": [
    "## some improvement in r. Lower F-stat. Interaction between volume and return is significant. volume itself ( including the two transformed volumes) is not significant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76217d1",
   "metadata": {},
   "source": [
    "## Out-of-sample testing\n",
    " # Split the data to train and test\n",
    " # standarizing train-data\n",
    " # use train parameters to standarize test-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8b58ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "syms1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed5a50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "c7594113",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from yahooquery import Ticker\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248bdcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fa5cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = syms1['next_ret']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0be642",
   "metadata": {},
   "outputs": [],
   "source": [
    "syms1_train, syms1_test, y_train, y_test = train_test_split(syms1, y, test_size=0.3, random_state= 12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf5630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vrs= syms1_train.columns\n",
    "vrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c814be7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e624aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "XY_train = syms1_train[vrs[4:]].dropna()\n",
    "XY_test = syms1_test[vrs[4:]].dropna()\n",
    "XY_train_standarized = (XY_train - XY_train.mean() )/XY_train.std() \n",
    "XY_test_standarized = (XY_test - XY_train.mean() )/XY_train.std()  # Note we standarize the test data with the \n",
    "X_train_standarized= XY_train_standarized[vrs[5:]]\n",
    "X_test_standarized = XY_test_standarized[vrs[5:]]\n",
    "y_train_standarized = XY_train_standarized['next_ret']\n",
    "y_test_standarized =   XY_test_standarized['next_ret']\n",
    "\n",
    "X_train_standarized = sm.add_constant(X_train_standarized)\n",
    "X_test_standarized = sm.add_constant(X_test_standarized)\n",
    "lm = sm.OLS(y_train_standarized, X_train_standarized).fit()\n",
    "XY_train_standarized['pred'] = lm.predict(X_train_standarized)\n",
    "XY_test_standarized['pred'] = lm.predict(X_test_standarized)\n",
    "r_train = XY_train_standarized['pred'].corr(XY_train_standarized['next_ret'])\n",
    "r_test = XY_test_standarized['pred'].corr(XY_test_standarized['next_ret'])\n",
    "\n",
    "print(  'r_train = = ' , r_train)\n",
    "print(  'r_test    =' , r_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084a73e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train_rsquared = ' ,f'{r_train**2:.8f}'  )\n",
    "print('test_rsquared = ' , f'{r_test**2:.8f}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8146000b",
   "metadata": {},
   "source": [
    "## It is lower than the r-square of the regression without splitting the sample. Hoowever, it is very close, due to the large sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bb502d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca727afc",
   "metadata": {},
   "source": [
    "## 2. Introduction to Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37361c19",
   "metadata": {},
   "source": [
    "## We should expect ANN to take care of non-linearity. \n",
    "## we will apply sequential ANN using raw basic variables without interactionsl\n",
    "  # Specifically [ 'ret' , spy_ret , etf_ret , volume ]\n",
    "  ## The point here see whether ANN can do all the work for us\n",
    "## We will run ANN with all created interactions and see how much ANN improves ove our model with our manually developed interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa56258e",
   "metadata": {},
   "source": [
    " ## First, we \n",
    "#  (i) will install and use two python library : tensorflow and keras\n",
    " # Provide a description of Keras sequantial NN parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa4df72",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea45124f",
   "metadata": {},
   "source": [
    "## A tensor is a multi-dimensional Array (including 2 or more dimesions)\n",
    "## TensorFlow is an open source library for numerical computation and large-scale machine learning. \n",
    " # It bundles Neural Network algorithms. \n",
    " # It uses python to provide a convenient front-end API for building models. However, those applications get executed  C++"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb4da3e",
   "metadata": {},
   "source": [
    "## TensorFlow allows creating data flow graphs. That is:\n",
    " # A structures that describe how data moves through a graph. \n",
    " # Each node in the graph represents a mathematical operation.\n",
    " # Each connection or edge between nodes is a multi-dimensional data array-- tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6279c9ed",
   "metadata": {},
   "source": [
    "## Import these keras libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c805bc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212b11e8",
   "metadata": {},
   "source": [
    "## With the “Sequential” we create a sequence of ANN layers stacked one after the other. \n",
    "# Each layer is defined using the “Dense” module of Keras.\n",
    "# Keras Dense layer contains all the neurons that are deeply connected within themselves. Every neuron in the dense layer takes the input from all the other neurons of the previous layer.  \n",
    "# We need to specify,\n",
    " # ( i) how many neurons would be there, \n",
    " # (Ii) How we initialize the weights in the network. \n",
    " # (iii) the activation function for each neuron in that layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e30fc70",
   "metadata": {},
   "source": [
    "## Parameters to be specified:\n",
    "  # 1. units=8 means we are creating a layer with 5 neirons. Each of of which will be receiving input values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befddbfb",
   "metadata": {},
   "source": [
    "## input_dim=4: \n",
    " # This means there are 4 predictors in the input data which is expected by the first layer. \n",
    " # The Sequential model passes this information further to the next layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c024125",
   "metadata": {},
   "source": [
    "\n",
    "## kernel_initializer=’normal’: \n",
    " # The algorithm has to decide the value for each weight. This parameter specifies that. You can choose different values for instead of ‘normal’ . The following link provides all the options:\n",
    " https://keras.io/api/layers/initializers/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1793f47",
   "metadata": {},
   "source": [
    "\n",
    "## activation=’relu’: This specifies the activation function for the calculations inside each neuron. You can choose values like ‘relu’, ‘tanh’, ‘sigmoid’, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121c2c3a",
   "metadata": {},
   "source": [
    "## batch_size=60: \n",
    " # This specifies how many rows will be passed to the Network in one go after which the SSE calculation will begin and the neural network will start adjusting its weights based on the errors.\n",
    " # When all the rows are passed in the batches of 60 rows each as specified in this parameter, then we call that 1-epoch. Or one full data cycle. \n",
    " # This is also known as mini-batch gradient descent. \n",
    " # A small value of batch_size will make the ANN look at the data slowly, \n",
    " # a large value will make the ANN look at the data fast which could lead to underfitting.\n",
    " # a proper value must be chosen using hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbcf0f7",
   "metadata": {},
   "source": [
    "## Epochs=50: \n",
    "## The same activity of adjusting weights continues for 50 times, as specified by this parameter. \n",
    "# That isthe ANN looks at the full training data 50 times and adjusts its weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da60cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f56cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "vrs[5:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8a96fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_basic_columns = [ 'ret' , 'volume' , 'spy_ret' , 'etf_ret' ]\n",
    "x_train = X_train_standarized[ann_basic_columns]\n",
    "x_test = X_test_standarized[ann_basic_columns]\n",
    "y_train = XY_train_standarized['next_ret']\n",
    "y_test = XY_test_standarized['next_ret']\n",
    "print (x_train.shape, x_test.shape , y_train.shape , y_test.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de2854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    " \n",
    "# create ANN model\n",
    "model = Sequential()\n",
    " \n",
    "# Defining the Input layer and FIRST hidden layer, both are same!\n",
    "model.add(Dense(units=12, input_dim= 4, kernel_initializer='normal', activation='relu'))\n",
    " \n",
    "# Defining the Second layer of the model\n",
    "# after the first layer we don't have to specify input_dim as keras configure it automatically\n",
    "model.add(Dense(units=4, kernel_initializer='normal', activation='relu'))\n",
    " \n",
    "# The output neuron is a single fully connected node \n",
    "# Since we will be predicting a single number\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    " \n",
    "# Compiling the model\n",
    "from sklearn.metrics import r2_score\n",
    "model.compile(loss='mse', optimizer='rmsprop', metrics=['cosine_proximity'], run_eagerly=True)\n",
    "# model.compile(loss='mean_squared_error', optimizer='adam')\n",
    " \n",
    "# Fitting the ANN to the Training set\n",
    "model.fit(x_train, y_train ,batch_size = 60, epochs = 50, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a50212",
   "metadata": {},
   "source": [
    "## cosine proximity is corelation centered arround zero. Actually correlation between x and y is cosine proximity centered arount (x_mean, y_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fcacb3",
   "metadata": {},
   "source": [
    "## Here we create a prediction column in the standarized-train-data and calculate the in-sample correlation coefficient between next_re and its prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c673e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_train_standarized ['basic_train_pred'] =  model.predict(x_train) \n",
    "XY_train_standarized ['basic_train_pred'].corr( XY_train_standarized ['next_ret'] )  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37349fc",
   "metadata": {},
   "source": [
    "## here we do the same for the out-of-sample correlation between next_ret and its prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5aaa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_test_standarized ['basic_test_pred'] =  model.predict(x_test) \n",
    "XY_test_standarized ['basic_test_pred'].corr( XY_test_standarized ['next_ret'] ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9e4869",
   "metadata": {},
   "source": [
    "## it lokas like ANN delivers better out-of-sample results without having to include the manual interactions in the inputs. ANN took care of the interactions itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdace22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c02fa69",
   "metadata": {},
   "source": [
    "## What if we specify interaction variables in the input ? \n",
    " # Can we further enhance the performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14148fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( vrs[5:] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff7c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_standarized.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d2b2df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aedfd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_columns = vrs[5:]\n",
    "x_train = X_train_standarized[ann_columns]\n",
    "x_test = X_test_standarized[ann_columns]\n",
    "y_train = XY_train_standarized['next_ret']\n",
    "y_test = XY_test_standarized['next_ret']\n",
    "print (x_train.shape, x_test.shape , y_train.shape , y_test.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0630bf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=12, input_dim= 9, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "model.add(Dense(units=4, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "model.compile(loss='mse', optimizer='rmsprop', metrics=['cosine_proximity'], run_eagerly=True)\n",
    "model.fit(x_train, y_train ,batch_size = 60, epochs = 50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c6537e",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_train_standarized ['ann_train_pred'] =  model.predict(x_train) \n",
    "XY_train_standarized ['ann_train_pred'].corr( XY_train_standarized ['next_ret'] )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02012326",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_test_standarized ['ann_test_pred'] =  model.predict(x_test) \n",
    "XY_test_standarized ['ann_test_pred'].corr( XY_test_standarized ['next_ret'] )  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d598b0",
   "metadata": {},
   "source": [
    "## Two conclusions\n",
    " # (i) the basic ( with 4 basic inputs)  Sequential ANN model does better than a linear regression that includes the interaction terms ( 9 inputs in total)\n",
    " # (ii) Adding the interaction variables to the inputs of the Sequential ANN did enhance out-of-sample performance. \n",
    " \n",
    " ## This testifies to the power of ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3709b40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "syms.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be3b99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vrs_all = ['symbol', 'etf', 'day', 'time','next_ret', 'ret',\n",
    "       'prev1_ret', 'prev2_ret', 'prev3_ret', 'prev4_ret', 'volume', 'mflow1',\n",
    "       'mflow2', 'mflow3', 'mflow4', 'spy_high', 'spy_low', \n",
    "       'spy_ret',  'spy_prev3_ret',\n",
    "       'spy_prev4_ret', 'spy_volume', 'spy_mflow1', 'spy_mflow2', 'spy_mflow3',\n",
    "       'spy_mflow4', 'etf_high', 'etf_low',  'etf_ret',\n",
    "       'etf_prev1_ret', 'etf_prev2_ret', 'etf_prev3_ret', 'etf_prev4_ret',\n",
    "       'etf_volume', 'etf_mflow1', 'etf_mflow2', 'etf_mflow3', 'etf_mflow4',\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6e8edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "syms1_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7db73a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "syms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5a5dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY = syms[vrs_all[4:]].dropna()\n",
    "X= XY[vrs_all[5:]]\n",
    "y = XY['next_ret']\n",
    "X = sm.add_constant(X)\n",
    "lm1 = sm.OLS(y, X).fit()\n",
    "lm1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0776cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ed291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = syms['next_ret']\n",
    "syms_train, syms_test, y_train, y_test = train_test_split(syms, y, test_size=0.3, random_state= 12345)\n",
    "XY_train = syms_train[vrs_all[4:]].dropna()\n",
    "XY_test = syms_test[vrs_all[4:]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92683aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_train = syms_train[vrs_all[4:]].dropna()\n",
    "XY_test = syms_test[vrs_all[4:]].dropna()\n",
    "XY_train_standarized = (XY_train - XY_train.mean() )/XY_train.std() \n",
    "XY_test_standarized = (XY_test - XY_train.mean() )/XY_train.std()  # Note we standarize the test data with the \n",
    "X_train_standarized= XY_train_standarized[vrs_all[5:]]\n",
    "X_test_standarized = XY_test_standarized[vrs_all[5:]]\n",
    "y_train_standarized = XY_train_standarized['next_ret']\n",
    "y_test_standarized =   XY_test_standarized['next_ret']\n",
    "\n",
    "y_train = XY_train_standarized['next_ret']\n",
    "y_test = XY_test_standarized['next_ret']\n",
    "\n",
    "X_train_standarized = sm.add_constant(X_train_standarized)\n",
    "X_test_standarized = sm.add_constant(X_test_standarized)\n",
    "lm = sm.OLS(y_train_standarized, X_train_standarized).fit()\n",
    "XY_train_standarized['pred'] = lm.predict(X_train_standarized)\n",
    "XY_test_standarized['pred'] = lm.predict(X_test_standarized)\n",
    "r_train = XY_train_standarized['pred'].corr(XY_train_standarized['next_ret'])\n",
    "r_test = XY_test_standarized['pred'].corr(XY_test_standarized['next_ret'])\n",
    "print(XY_train_standarized.shape , XY_test_standarized.shape)\n",
    "print(  'r_train = = ' , r_train)\n",
    "print(  'r_test    =' , r_test )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72964bb",
   "metadata": {},
   "source": [
    "## This provides imprtovement over the earlier 9-independent variables regression model that used onlu current information. The variables here information go back four 5-minutes periods.  # It captures past returns and money flow.\n",
    "# money-flow is defined here as return times volume. It is actually interaction of volume and returns for the symbol, its etf and spy\n",
    "# A more accurate definition for money flow is price change times volume. Here it is volume times percentage price changes (i.e. return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad0d68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_standarized.columns)\n",
    "print ( X_train_standarized.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8fb183",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(XY_train_standarized.columns )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976173cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(XY_test_standarized.columns )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd57d26",
   "metadata": {},
   "source": [
    "## Before we start, we can transform the standarized next_ret and its prediction back to their original values.\n",
    " # Recall that we divided subtracted the XY_train['next_ret'].mean() and divided by XY_train['next_ret'].std() . The reverse transformation would  multiplying by std and adding the mean. The followin renames XY_?? data frames and calculate original values for next_ret and apply the same transformation to the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb73797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LM32_XY_train = XY_train_standarized\n",
    "LM32_XY_test = XY_test_standarized\n",
    "next_ret_train_mean = XY_train['next_ret'].mean()\n",
    "next_ret_train_std = XY_train['next_ret'].std()\n",
    "LM32_XY_train['original_next_ret'] = next_ret_train_std *LM32_XY_train['next_ret']+ next_ret_train_mean \n",
    "LM32_XY_train['original_scale_pred'] = next_ret_train_std *LM32_XY_train['pred']+ next_ret_train_mean \n",
    "\n",
    "LM32_XY_test['original_next_ret'] = next_ret_train_std *LM32_XY_test['next_ret']+ next_ret_train_mean \n",
    "LM32_XY_test['original_scale_pred'] = next_ret_train_std *LM32_XY_test['pred']+ next_ret_train_mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e65f040",
   "metadata": {},
   "outputs": [],
   "source": [
    "LM32_XY_train[['next_ret',  'pred', 'original_next_ret', 'original_scale_pred'] ].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e124216",
   "metadata": {},
   "outputs": [],
   "source": [
    "LM32_XY_test['pred_Quantile'] = pd.qcut(LM32_XY_test.pred,5 ,        \n",
    "        labels=['Q1',\n",
    "                'Q2',\n",
    "                'Q3',\n",
    "                'Q4', 'Q5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2dec225",
   "metadata": {},
   "outputs": [],
   "source": [
    "LM32_next_ret_byQ = LM32_XY_test.groupby('pred_Quantile')['original_next_ret'].agg(['mean', 'std', 'count'])\n",
    "LM32_next_ret_byQ['t_stat'] = LM32_next_ret_byQ['mean']/(LM32_next_ret_byQ['std']/LM32_next_ret_byQ['count']**0.5)\n",
    "LM32_next_ret_byQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958e0daf",
   "metadata": {},
   "source": [
    "## Note that the 'pred' std is much smaller than next_ret-std. This carries on after the transformation\n",
    " # The scale of the prediction is lower as long as the regressions r-square is less than 1. The samler r_square the larger the scale disparity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024888b1",
   "metadata": {},
   "source": [
    "## Comments on trading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066beba8",
   "metadata": {},
   "source": [
    "## If you short the first and second quatimes and long the 4th and the fifth quatile you can make money\n",
    " # There are 78 5-minutes window in a day. we can actually calculate the daily return and approximate daily std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996b566f",
   "metadata": {},
   "source": [
    "## Assume thatwe we short $1 of quatile 1 and 2 and long $1 in quantile 4 and 45 the average 5-minutes return is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9854e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "annual_avg_ret = ( ( -LM32_next_ret_byQ['mean'][0] - LM32_next_ret_byQ['mean'][1] +\n",
    "   LM32_next_ret_byQ['mean'][3] + LM32_next_ret_byQ['mean'][4] )/4 )*78*252\n",
    "annual_std_ret =np.sqrt (   ( LM32_next_ret_byQ['std'][0]**2 + LM32_next_ret_byQ['std'][1]**2 +\n",
    "   LM32_next_ret_byQ['std'][3]**2 + LM32_next_ret_byQ['std'][4]**2 )/4  )*np.sqrt(78*252)\n",
    "print( 'annual_avg_ret = ',  annual_avg_ret  )\n",
    "print( 'annual_std_ret = ',  annual_std_ret  )  \n",
    "print( 'sharpe-ratio = ' ,  annual_avg_ret/annual_std_ret )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3992e6",
   "metadata": {},
   "source": [
    "## This can be optimistic, it ignores transaction cost and assumes that you can trade at the begining of the 5-minutes windo at the open price and set at the close price that window.\n",
    " # a more accurate analysis require simulation testing and an inventory management strategy.  Such staretegy may decide to seel or keep the position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a54606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc99202",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77de4cbf",
   "metadata": {},
   "source": [
    "## Can ANN do a better job if we feed it the same inputs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0fcfb9",
   "metadata": {},
   "source": [
    "## The following code replaces XY_?? data frames. However, their information is in LM32_??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9e3c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vrs_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78137b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_train = syms_train[vrs_all[4:]].dropna()\n",
    "XY_test = syms_test[vrs_all[4:]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2061361",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_train = syms_train[vrs_all[4:]].dropna()\n",
    "XY_test = syms_test[vrs_all[4:]].dropna()\n",
    "ann_columns = vrs_all[5:]\n",
    "x_train = X_train_standarized[ann_columns]\n",
    "x_test = X_test_standarized[ann_columns]\n",
    "y_train = XY_train_standarized['next_ret']\n",
    "y_test = XY_test_standarized['next_ret']\n",
    "print (x_train.shape, x_test.shape , y_train.shape , y_test.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b157bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d55f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units= 40, input_dim= 32, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "model.add(Dense(units=10, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "model.compile(loss='mse', optimizer='rmsprop', metrics=['cosine_proximity'], run_eagerly=True)\n",
    "model.fit(x_train, y_train ,batch_size = 200, epochs = 10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e909f329",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_train_standarized ['ann_train_pred'] =  model.predict(x_train) \n",
    "XY_train_standarized ['ann_train_pred'].corr( XY_train_standarized ['next_ret'] )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3812bf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_test_standarized ['ann_test_pred'] =  model.predict(x_test) \n",
    "XY_test_standarized ['ann_test_pred'].corr( XY_test_standarized ['next_ret'] ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69fc60d",
   "metadata": {},
   "source": [
    "## Very impressive performance. The out of sample correlation between actual and prediction   is almost three times its linear model with ellaborite manual interactions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc045ae2",
   "metadata": {},
   "source": [
    "## Note howver, that without developing these interactions, the ANN performance is rather limited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2abed9",
   "metadata": {},
   "source": [
    "## The conclusion here is that you need to study and understand the problem, engineer features from such understanding and then apply ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0bad30",
   "metadata": {},
   "source": [
    "## This ANN can be optimized with hyper-parameter calibration and more data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bf7367",
   "metadata": {},
   "source": [
    "## Impact on trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980b4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "XY_test_standarized.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dc7993",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN32_XY_train = XY_train_standarized\n",
    "ANN32_XY_test = XY_test_standarized\n",
    "next_ret_train_mean = XY_train['next_ret'].mean()\n",
    "next_ret_train_std = XY_train['next_ret'].std()\n",
    "ANN32_XY_train['original_next_ret'] = next_ret_train_std *ANN32_XY_train['next_ret']+ next_ret_train_mean \n",
    "ANN32_XY_train['original_scale_pred'] = next_ret_train_std *ANN32_XY_train['pred']+ next_ret_train_mean \n",
    "\n",
    "ANN32_XY_test['original_next_ret'] = next_ret_train_std *ANN32_XY_test['next_ret']+ next_ret_train_mean \n",
    "ANN32_XY_test['original_scale_pred'] = next_ret_train_std *ANN32_XY_test['pred']+ next_ret_train_mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58950de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN32_next_ret_byQ = ANN32_XY_test.groupby('pred_Quantile')['original_next_ret'].agg(['mean', 'std', 'count'])\n",
    "ANN32_next_ret_byQ['t_stat'] = ANN32_next_ret_byQ['mean']/(ANN32_next_ret_byQ['std']/np.sqrt(ANN32_next_ret_byQ['count']) )\n",
    "ANN32_next_ret_byQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4879586",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN32_XY_test['pred_Quantile'] = pd.qcut(ANN32_XY_test.ann_test_pred,5 ,        \n",
    "        labels=['Q1',\n",
    "                'Q2',\n",
    "                'Q3',\n",
    "                'Q4', 'Q5'])\n",
    "annual_avg_ret = ( ( -ANN32_next_ret_byQ['mean'][0] - ANN32_next_ret_byQ['mean'][1] +\n",
    "   ANN32_next_ret_byQ['mean'][3] + ANN32_next_ret_byQ['mean'][4] )/4 )*78*252\n",
    "annual_std_ret =np.sqrt (   ( ANN32_next_ret_byQ['std'][0]**2 + ANN32_next_ret_byQ['std'][1]**2 +\n",
    "   ANN32_next_ret_byQ['std'][3]**2 + ANN32_next_ret_byQ['std'][4]**2 )/4  )*np.sqrt(78*252)\n",
    "print( 'annual_avg_ret = ',  annual_avg_ret  )\n",
    "print( 'annual_std_ret = ',  annual_std_ret  )  \n",
    "print( 'sharpe-ratio = ' ,  annual_avg_ret/annual_std_ret )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d082572d",
   "metadata": {},
   "source": [
    "## The ANN does provide much better psueudo trading results. 219% instead of 80% return with similar risk. It triples the sharpe ration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170954ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fe3973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32635eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ee3837",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
